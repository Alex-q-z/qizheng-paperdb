# Machine Learning Systems

## LLM Systems: Improving Inference Serving Efficiency

* DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving, OSDI 2024 [[paper]](https://arxiv.org/abs/2401.09670)

* Parrot: Efficient Serving of LLM-based Applications with Semantic Variable, OSDI 2024 [[paper]](https://arxiv.org/pdf/2405.19888)

## LLM Systems: Compound AI Systems

* Caravan: Practical Online Learning of In-Network ML Models with Labeling Agents, OSDI 2024 [[paper]](https://www.usenix.org/system/files/osdi24-zhang-qizheng.pdf)

## LLM Systems: KV Cache and Memory Management

* FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU, ICML 2023 oral [[paper]](https://arxiv.org/abs/2303.06865)

* Efficient Memory Management for Large Language Model Serving with PagedAttention, SOSP 2023 [[paper]](https://arxiv.org/abs/2309.06180)

* CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving, SIGCOMM 2024 [[paper]](https://arxiv.org/abs/2310.07240)

* CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion, EuroSys 2025 [[paper]](https://arxiv.org/abs/2405.16444)
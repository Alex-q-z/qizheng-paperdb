# Large Language Models

## Inference-Time Scaling

Can we leverage inference-time compute scaling techniques, like best-of-n sampling or self-refinement, to achieve better model performance?

* Large Language Monkeys: Scaling Inference Compute with Repeated Sampling, arXiv:2407.21787v2 [[paper]](https://arxiv.org/abs/2407.21787)

## Speculative Decoding

## LLMs + Programming

Improving LLM capabilities through generating intermediate computer programs.

* PAL: Program-aided Language Models [[paper]](https://arxiv.org/abs/2211.10435)

* The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators, NeurIPS 2024 [[paper]](https://arxiv.org/abs/2407.11004)

## Compound AI Systems: Optimization

Design and optimization of AI systems where more than one LLM is used.

* TextGrad: Automatic "Differentiation" via Text, arXiv:2406.07496v1 [[paper]](https://arxiv.org/pdf/2406.07496)

## Compound AI Systems: Programming

* DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines, ICLR 2024 spotlight [[paper]](https://arxiv.org/abs/2310.03714)

## Weak Supervision

* Your Weak LLM is Secretly a Strong Teacher for Alignment, arXiv:2409.08813v1 [[paper]](https://arxiv.org/pdf/2409.08813)
